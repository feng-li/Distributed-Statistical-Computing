{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Spark基础\n",
    "\n",
    "\n",
    "李丰\n",
    "\n",
    "feng.li@cufe.edu.cn\n",
    "\n",
    "中央财经大学"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Spark的优势\n",
    "\n",
    "- Apache Spark是一个开源集群运算框架，最初是由加州大学柏克莱分校AMPLab所开发。\n",
    "- Spark使用了存储器内运算技术，能在数据尚未写入硬盘时即在存储器内分析运算。\n",
    "- Spark在存储器内运行程序的运算速度能做到比Hadoop MapReduce的运算速度快上100倍。\n",
    "- 即便是运行程序于硬盘时，Spark也能快上10倍速度。\n",
    "- Spark允许用户将数据加载至集群存储器，并多次对其进行查询，非常适合用于机器学习算法。\n",
    "- 支持 Java, Scala, Python 和 R."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Spark 组件\n",
    "\n",
    "- **DataFrame** 分布式数据框。\n",
    "- **Spark Streaming** 利用Spark快速调度能力截取小批量的数据并对之运行RDD转换运行流分析。\n",
    "- **MLlib** 使用许多常见的机器学习和统计算法（分类与回归，降维、特征提取、优化算法）简化大规模机器学习时间。\n",
    "- **GraphX** Spark上的分布式图形处理框架。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Spark RDD\n",
    "\n",
    "- RDD是Spark的核心数据抽象方式，全称为Resillient Distributed Dataset，即弹性分布式数据集。\n",
    "- RDD通常通过Hadoop上的文件，即HDFS文件或者Hive表，来进行创建；有时也可以通过应用程序中的集合来创建。\n",
    "- RDD的数据默认情况下存放在内存中的，但是在内存资源不足时，Spark会自动将RDD数据写入磁盘（弹性）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Spark 架构\n",
    "\n",
    "![Spark](./figures/spark-cluster-overview.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Spark 运行模式\n",
    "\n",
    "- Spark 可以作为一个独立的分布式系统运行。\n",
    "- Spark 也可以运行在Hadoop等分布式系统中。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Spark交互模式\n",
    "\n",
    "- Spark-shell 基于Scala语言的交互界面\n",
    "\n",
    "    $ spark-shell\n",
    "    \n",
    "- PySpark Python版本的Spark交互\n",
    "\n",
    "    $ pyspark \n",
    "    \n",
    "- SparkR   R版本的Spark交互模式\n",
    "\n",
    "    $ sparkR\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Spark Batch模式\n",
    "\n",
    "    $ spark-submit <SparkApp.py>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "###  通过 Jupyter (Python Kernel) 运行PySpark\n",
    "\n",
    "- 首先安装Python `findspark` 模块，使得`pyspark`可以作为Python的一个标准模块调用。\n",
    "- Spark的安装路径可以通过环境变量`$SPARK_HOME`得到。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init(\"/usr/lib/spark-current\")\n",
    "import pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 创建一个Spark应用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "sc.stop()\n",
    "sc = pyspark.SparkContext(\"local\", \"My First Spark App\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- 注意：不能同时创建多个SparkContext\n",
    "- 要么结束上一个SparkContext再创建新的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- 或者使用一个可容错的函数代替"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "sc = pyspark.SparkContext.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Spark提供了非常简洁的与RDD交互的方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Copyright (c) 2000-2005 INRIA, France Telecom'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textFile = sc.textFile(\"license.txt\")\n",
    "textFile.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textFile.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Spark的MapReduce就像Python的lambda函数一样简单"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textFile.map(lambda line: len(line.split()))\\\n",
    "        .reduce(lambda a, b: a if (a > b) else b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Spark允许将数据加载到分布式系统的共享缓存中以加快数据访问"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "license.txt MapPartitionsRDD[1] at textFile at NativeMethodAccessorImpl.java:0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textFile.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textFile.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Spark数据类型\n",
    "\n",
    "- 存储于单机的本地向量（Local vector）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.sparse as sps\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "\n",
    "# Use a NumPy array as a dense vector.\n",
    "dv1 = np.array([1.0, 0.0, 3.0])\n",
    "# Use a Python list as a dense vector.\n",
    "dv2 = [1.0, 0.0, 3.0]\n",
    "# Create a SparseVector.\n",
    "sv1 = Vectors.sparse(3, [0, 2], [1.0, 3.0])\n",
    "# Use a single-column SciPy csc_matrix as a sparse vector.\n",
    "sv2 = sps.csc_matrix((np.array([1.0, 3.0]),\n",
    "                      np.array([0, 2]),\n",
    "                      np.array([0, 2])), shape=(3, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- 存储于单机的本地矩阵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DenseMatrix([[1., 4.],\n",
      "             [2., 5.],\n",
      "             [3., 6.]])\n",
      "3 X 2 CSCMatrix\n",
      "(0,0) 9.0\n",
      "(2,1) 6.0\n",
      "(1,1) 8.0\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.linalg import Matrix, Matrices\n",
    "# Create a dense matrix ((1.0, 2.0), (3.0, 4.0), (5.0, 6.0))\n",
    "dm2 = Matrices.dense(3, 2, [1, 2, 3, 4, 5, 6])\n",
    "print(dm2)\n",
    "# Create a sparse matrix ((9.0, 0.0), (0.0, 8.0), (0.0, 6.0))\n",
    "sm = Matrices.sparse(3, 2, [0, 1, 3], [0, 2, 1], [9, 6, 8])\n",
    "print(sm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- 标签（Labeled Points）主要用于有监督的机器学习"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.linalg import SparseVector\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "\n",
    "# Create a labeled point with a positive label and a dense feature vector.\n",
    "pos = LabeledPoint(1.0, [1.0, 0.0, 3.0])\n",
    "\n",
    "# Create a labeled point with a negative label and a sparse feature vector.\n",
    "neg = LabeledPoint(0.0, SparseVector(3, [0, 2], [1.0, 3.0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- 稀疏数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PythonRDD[9] at RDD at PythonRDD.scala:49\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.util import MLUtils\n",
    "examples = MLUtils.loadLibSVMFile(sc,\n",
    "           \"libsvm_data.txt\")\n",
    "print(examples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 分布式矩阵 （Distributed matrix）\n",
    "\n",
    "#### 行矩阵（RowMatrix）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 3 MapPartitionsRDD[13] at mapPartitions at PythonMLLibAPI.scala:1339\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.linalg.distributed import RowMatrix\n",
    "# Create an RDD of vectors.\n",
    "rows = sc.parallelize([[1, 2, 3], [4, 5, 6], [7, 8, 9], [10, 11, 12]])\n",
    "# Create a RowMatrix from an RDD of vectors.\n",
    "mat = RowMatrix(rows)\n",
    "# Get its size.\n",
    "m = mat.numRows()  # 4\n",
    "n = mat.numCols()  # 3\n",
    "# Get the rows as an RDD of vectors again.\n",
    "rowsRDD = mat.rows\n",
    "print(m,n,rowsRDD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### 有行号的行矩阵（IndexedRowMatrix）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.linalg.distributed import IndexedRow, IndexedRowMatrix\n",
    "# Create an RDD of indexed rows.\n",
    "#   - This can be done explicitly with the IndexedRow class:\n",
    "indexedRows = sc.parallelize([IndexedRow(0, [1, 2, 3]),\n",
    "                              IndexedRow(1, [4, 5, 6]),\n",
    "                              IndexedRow(2, [7, 8, 9]),\n",
    "                              IndexedRow(3, [10, 11, 12])])\n",
    "#   - or by using (long, vector) tuples:\n",
    "indexedRows = sc.parallelize([(0, [1, 2, 3]), (1, [4, 5, 6]),\n",
    "                              (2, [7, 8, 9]), (3, [10, 11, 12])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 上机实践\n",
    "\n",
    "- 创建并运行一个简单的Spark应用。\n",
    "\n",
    "- 了解Spark的[基础数据类型](https://spark.apache.org/docs/latest/mllib-data-types.html)。\n",
    "\n",
    "- 课外阅读材料\n",
    "\n",
    "    - https://spark.apache.org/docs/latest/api/python/index.html\n",
    "    - https://spark.apache.org/examples.html"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
