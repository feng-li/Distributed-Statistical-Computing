{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Distributed Database\n",
    "\n",
    "\n",
    "\n",
    "## Feng Li\n",
    "\n",
    "### Central University of Finance and Economics\n",
    "\n",
    "### [feng.li@cufe.edu.cn](feng.li@cufe.edu.cn)\n",
    "### Course home page: [https://feng.li/distcomp](https://feng.li/distcomp)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Why Hive?\n",
    "\n",
    "- Many of those low-level details are actually quite repetitive from one job to the next, from low-level chores like wiring together Mappers and Reducers to certain data manipulation constructs, like filtering for just the data you want and performing SQL-like joins on data sets.\n",
    "\n",
    "- Hive not only provides a familiar programming model for people who know SQL, it also eliminates lots of boilerplate and sometimes-tricky coding you would have to do in Hadoop."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![Hive-Modules](./figures/hive-modules.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## How does Hive work?\n",
    "\n",
    "\n",
    "- When MapReduce jobs are required, Hive doesn't generate Java MapReduce programs.    \n",
    "\n",
    "- Instead, it uses built-in, generic Mapper and Reducer modules that are driven by an XML file representing the **job plan**\n",
    "\n",
    "- In other words, these generic modules function like mini language interpreters and the **language** to drive the computation is encoded in XML."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Hive Batch\n",
    "\n",
    "\n",
    "- Run hive commands from the termial\n",
    "\n",
    "    $ hive -e \"dfs -ls /;\"\n",
    "    \n",
    "- Run Hive scripts from the termimal\n",
    "\n",
    "    $ hive -f /path/to/file/withqueries.hql"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Hive Interactive\n",
    "\n",
    "- Start Hive from a Terminal\n",
    "\n",
    "    $ hive\n",
    "\n",
    "- Execute command within Hive \n",
    "\n",
    "    hive> dfs -ls /;\n",
    "    \n",
    "- Exit Hive\n",
    "\n",
    "    hive> exit;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Hive (via Jupyter HiveQL)\n",
    "\n",
    "\n",
    "If you want to run Hive within Jupyter with HiveQL，you have to connect to the Hive Server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "create_engine('hive://localhost', )\n",
      "Connection established to database!\n"
     ]
    }
   ],
   "source": [
    "$$ url=hive://localhost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Then you are in Hive Shell. You chould try HDFS commands as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DFS Output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Found 7 items</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>drwxr-x--x   - hadoop    hadoop          0 201...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>drwxrwxrwx   - flowagent hadoop          0 201...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>drwxr-x--x   - student   hadoop          0 201...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>drwxr-x--x   - hadoop    hadoop          0 201...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>drwxrwxrwx   - root      hadoop          0 201...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>drwxr-x--t   - hadoop    hadoop          0 201...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-rw-r-----   2 student   hadoop   19904316 201...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ]
     },
     "execution_count": 2,
     "metadata": {
      "image/png": {
       "height": 480,
       "width": 640
      }
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfs -ls /;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Hive with Database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Show Databases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>database_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>default</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mydb</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ]
     },
     "execution_count": 3,
     "metadata": {
      "image/png": {
       "height": 480,
       "width": 640
      }
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SHOW DATABASES;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>database_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>default</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ]
     },
     "execution_count": 4,
     "metadata": {
      "image/png": {
       "height": 480,
       "width": 640
      }
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SHOW DATABASES Like 'd*';"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Create a Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table created!"
     ]
    }
   ],
   "source": [
    "CREATE DATABASE IF NOT EXISTS mydb;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table created!"
     ]
    }
   ],
   "source": [
    "CREATE DATABASE IF NOT EXISTS financials LOCATION '/user/lifeng/hive';"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>database_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>default</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>financials</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>mydb</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ]
     },
     "execution_count": 7,
     "metadata": {
      "image/png": {
       "height": 480,
       "width": 640
      }
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SHOW DATABASES;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Drop a database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "DROP DATABASE IF EXISTS financials;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>database_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>default</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mydb</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ]
     },
     "execution_count": 9,
     "metadata": {
      "image/png": {
       "height": 480,
       "width": 640
      }
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SHOW DATABASES;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Use some database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "USE mydb;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Create a table within the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table created!"
     ]
    }
   ],
   "source": [
    "CREATE TABLE IF NOT EXISTS mydb.employees (\n",
    "    name\n",
    "        STRING COMMENT 'Employee name',\n",
    "    salary\n",
    "        FLOAT COMMENT 'Employee salary',\n",
    "    subordinates ARRAY<STRING> COMMENT 'Names of subordinates',\n",
    "    deductions MAP<STRING, FLOAT>\n",
    "        COMMENT 'Keys are deductions names, values are percentages',\n",
    "    address\n",
    "        STRUCT<street:STRING, city:STRING, state:STRING, zip:INT>\n",
    "        COMMENT 'Home address')\n",
    "COMMENT 'Description of the table'\n",
    "TBLPROPERTIES ('creator'='me', 'created_at'='2012-01-02 10:00:00');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tab_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>employees</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ]
     },
     "execution_count": 14,
     "metadata": {
      "image/png": {
       "height": 480,
       "width": 640
      }
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SHOW TABLES;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Create an external table\n",
    "\n",
    "Assume we have a data file `stocks.txt` located in HDFS at `/user/lifeng/data`, we could connect it with Hive as an external table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table created!"
     ]
    }
   ],
   "source": [
    "create external table if not exists stocks (\n",
    "    symbol string, \n",
    "    ymd string, \n",
    "    price_open float, \n",
    "    price_high float, \n",
    "    price_low float, \n",
    "    price_close float, \n",
    "    volume int, \n",
    "    price_adj_close float )\n",
    "row format delimited fields terminated by ',' \n",
    "location '/user/lifeng/data';"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "If you have an external file with a header/footer, you could exclude it with \n",
    "\n",
    "\n",
    "    TBLPROPERTIES('skip.header.line.count'='1', 'skip.footer.line.count'='2');\n",
    "    \n",
    "when you create the table."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Basic Statistics with Hive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_c0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>69.363</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ]
     },
     "execution_count": 22,
     "metadata": {
      "image/png": {
       "height": 480,
       "width": 640
      }
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SELECT avg(price_close) FROM stocks WHERE symbol = 'AAPL';"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Alternatives to Hive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Pig\n",
    "\n",
    "- Suppose you have one or more sources of input data and you need to perform a complex set of transformations to generate one or more collections of output data.\n",
    "\n",
    "- Pig is described as a data flow language, rather than a query language. In Pig, you write a series of declarative statements that define relations from other relations, where each new relation performs some new data transformation. Pig looks at these declarations and then **builds up a sequence of MapReduce jobs** to perform the transformations until the final results are computed the way that you want.\n",
    "\n",
    "- A drawback of Pig is that it uses a custom language not based on SQL. \n",
    "\n",
    "- See th Pig home page https://pig.apache.org/ for more information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## HBase\n",
    "\n",
    "- HBase is inspired by Google’s Big Table.\n",
    "\n",
    "- It provides distributed and scalable data store that supports row-level updates, rapid queries, and row-level transactions (but not multirow transactions)\n",
    "\n",
    "- HBase uses HDFS for durable file storage of data.\n",
    "\n",
    "- HBase also uses in-memory caching of data.\n",
    "\n",
    "- HBase doesn’t provide a query language like SQL, but Hive is now integrated with HBase.\n",
    "\n",
    "- See the HBase homepage https://hbase.apache.org/ for more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Lab 4\n",
    "\n",
    "- Create an external table with Hive for the data `airdelay_small.csv`\n",
    "\n",
    "- Use the Hive internal function https://www.tutorialspoint.com/hive/hive_built_in_functions.htm do basic statistic as we had with Hadoop.\n",
    "\n",
    "- External Reading\n",
    "\n",
    "    Capriolo, Edward, Dean Wampler, and Jason Rutherglen. Programming Hive: Data warehouse and query language for Hadoop. ” O’Reilly Media, Inc.”, 2012."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "HiveQL",
   "language": "hiveql",
   "name": "hiveql"
  },
  "language_info": {
   "codemirror_mode": "sql",
   "file_extension": ".hiveql",
   "mimetype": "text/x-hive",
   "name": "hive",
   "pygments_lexer": "postgresql"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
